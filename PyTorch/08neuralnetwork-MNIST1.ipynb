{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dataset = pd.read_csv(\"./fashion-mnist-train.csv\")\r\n",
    "print(len(dataset))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "targets_cls = dataset.pop(\"label\")\r\n",
    "\r\n",
    "inputs = np.array(dataset)\r\n",
    "targets_cls = np.array(targets_cls)\r\n",
    "\r\n",
    "\r\n",
    "targets = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for _ in targets_cls:\r\n",
    "    x = np.zeros(10)\r\n",
    "    x[_] = 1\r\n",
    "    targets.append(x) \r\n",
    "\r\n",
    "targets = np.array(targets)\r\n",
    "inputs = inputs/255\r\n",
    "\r\n",
    "print(inputs.shape, targets.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs, targets, test_size= 0.2, random_state=123)\r\n",
    "\r\n",
    "print(inputs_train.shape, targets_train.shape, inputs_test.shape, targets_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(48000, 784) (48000, 10) (12000, 784) (12000, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "inputs_train = torch.from_numpy(inputs_train.astype(np.float32)) \r\n",
    "inputs_test = torch.from_numpy(inputs_test.astype(np.float32)) \r\n",
    "targets_train = torch.from_numpy(targets_train.astype(np.float32)) \r\n",
    "targets_test = torch.from_numpy(targets_test.astype(np.float32)) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "input_train_dl = DataLoader(inputs_train,10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class FashionModel(torch.nn.Module):\r\n",
    "    def __init__(self,inputs_dim, layer_dim , output_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.linear1 = torch.nn.Linear(inputs_dim, layer_dim)\r\n",
    "        self.linear2 = torch.nn.Linear(layer_dim, output_dim)\r\n",
    "    \r\n",
    "    def forward(self,inputs):\r\n",
    "        self.output1 = torch.relu(self.linear1(inputs))\r\n",
    "        targets_pred = torch.softmax(self.linear2(self.output1),dim=-1)\r\n",
    "        \r\n",
    "        return(targets_pred)\r\n",
    "\r\n",
    "class FashionModel2(torch.nn.Module):\r\n",
    "    def __init__(self,inputs_dim, output_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = torch.nn.Linear(inputs_dim, output_dim)\r\n",
    "    \r\n",
    "    def forward(self,inputs):\r\n",
    "        targets_pred = torch.softmax(self.linear(inputs),dim=-1)\r\n",
    "        \r\n",
    "        return(targets_pred)\r\n",
    "\r\n",
    "FashionNetwork = FashionModel(784, 64, 10)\r\n",
    "# FashionNetwork = FashionModel2(784,10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "loss_fn = torch.nn.BCELoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "opt = torch.optim.SGD(FashionNetwork.parameters(), lr = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "epochs = 300\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    targets_pred = FashionNetwork(inputs_train)\r\n",
    "    loss = loss_fn(targets_pred, targets_train)\r\n",
    "    \r\n",
    "    loss.backward()\r\n",
    "    \r\n",
    "    opt.step()\r\n",
    "    opt.zero_grad()\r\n",
    "    \r\n",
    "    if (epoch+1)%10 == 0:\r\n",
    "       print(\"Epoch: \", epoch+1, \"  Loss: \", round(loss.item(),4))\r\n",
    "    \r\n",
    "    if (epoch+1)%50 == 0:\r\n",
    "        with torch.no_grad():\r\n",
    "        \r\n",
    "            targets_test_pred = FashionNetwork(inputs_test)\r\n",
    "            targets_test_cls = targets_test_pred.detach().clone()\r\n",
    "\r\n",
    "            for i in range(len(targets_test_cls)):\r\n",
    "                for j in range(len(targets_test_cls[i])):\r\n",
    "                    if targets_test_cls[i][j] == max(targets_test_cls[i]):\r\n",
    "                        targets_test_cls[i][j] = 1\r\n",
    "                    else:\r\n",
    "                        targets_test_cls[i][j] = 0\r\n",
    "            \r\n",
    "            targets_test_temp = targets_test.detach().clone()\r\n",
    "\r\n",
    "            \r\n",
    "            correct = 0\r\n",
    "            \r\n",
    "            for i in range(len(targets_test_temp)):\r\n",
    "                if targets_test_temp[i].eq(targets_test_cls[i]).sum()==10:\r\n",
    "                    correct += 1\r\n",
    "            \r\n",
    "            acc = correct/len(targets_test)\r\n",
    "            accuracy = round(acc*100)\r\n",
    "            \r\n",
    "            print(\"Accuracy : \",accuracy,\"%\")\r\n",
    "            \r\n",
    "            if accuracy>90:\r\n",
    "                break "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-77cd860eae83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtargets_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFashionNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1121\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2824\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "index = 1111\r\n",
    "\r\n",
    "test = inputs[index]\r\n",
    "\r\n",
    "img_arr = test.reshape(28,28)\r\n",
    "plt.imshow(img_arr, cmap=\"gray\")\r\n",
    "\r\n",
    "test = torch.from_numpy(test.reshape(1,-1).astype(np.float32))\r\n",
    "\r\n",
    "pred = FashionNetwork(test).detach().numpy()\r\n",
    "pred_cls = np.where(pred==pred.max())\r\n",
    "print(\"Prediction: \",pred_cls[1][0])\r\n",
    "print(\"Target: \",targets_cls[index])\r\n",
    "\r\n",
    "print(\"\"\"\r\n",
    "0 T-shirt/top, \r\n",
    "1 Trouser, \r\n",
    "2 Pullover,\r\n",
    "3 Dress,\r\n",
    "4 Coat,\r\n",
    "5 Sandal,\r\n",
    "6 Shirt,\r\n",
    "7 Sneaker,\r\n",
    "8 Bag,\r\n",
    "9 Ankle boot\r\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:  9\n",
      "Target:  9\n",
      "\n",
      "0 T-shirt/top, 1 Trouser, \n",
      "2 Pullover,\n",
      "3 Dress,\n",
      "4 Coat,\n",
      "5 Sandal,\n",
      "6 Shirt,\n",
      "7 Sneaker,\n",
      "8 Bag,\n",
      "9 Ankle boot\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "2f97e602bf3efd1202fb41503b5059ddd336053f3958a92ca7b4a7a1e22459bb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}