{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#importing useful modules\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#loading csv data in variables\r\n",
    "\r\n",
    "train_data = np.array(pd.read_csv(\"./fashion-mnist-train.csv\")).astype(np.float32)\r\n",
    "test_data = np.array(pd.read_csv(\"./fashion-mnist-test.csv\")).astype(np.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#converting data to tensors\r\n",
    "\r\n",
    "train_data = torch.from_numpy(train_data)\r\n",
    "test_data = torch.from_numpy(test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#creating dataloader objects to load data in batches of 100 to save cpu power and time\r\n",
    "\r\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True)\r\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#making Neural network using layered linear model\r\n",
    "#using relu func to erase linearity in model\r\n",
    "#using softmax to change predictions into probabilities\r\n",
    "\r\n",
    "class FashionNetwork(torch.nn.Module):\r\n",
    "    def __init__(self, input_dim, layer_dim, out_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.linear1 = torch.nn.Linear(input_dim, layer_dim)\r\n",
    "        self.linear2 = torch.nn.Linear(layer_dim, out_dim)\r\n",
    "    \r\n",
    "    def forward(self, inputs):\r\n",
    "        inputs = inputs/255\r\n",
    "        out = self.linear1(inputs)\r\n",
    "        out = torch.relu(out)\r\n",
    "        out = self.linear2(out)\r\n",
    "        targets_pred = torch.softmax(out, dim=-1)\r\n",
    "        \r\n",
    "        return targets_pred\r\n",
    "    \r\n",
    "\r\n",
    "model = FashionNetwork(784,100,10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#using binary cross entropy function to calculate loss\r\n",
    "\r\n",
    "loss_fn = torch.nn.BCELoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#optimizer to do gradient decent\r\n",
    "\r\n",
    "opt = torch.optim.SGD(model.parameters(), lr = 0.5)\r\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=0.005)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Training Loop\r\n",
    "\r\n",
    "epochs = 3\r\n",
    "for epoch in range(epochs):\r\n",
    "    for batch in train_loader:\r\n",
    "        \r\n",
    "        #taking inputs out of batch of the training dataset\r\n",
    "        inputs = batch[:,1:785]\r\n",
    "        \r\n",
    "        #one hot encoding the targets after taking them out of the training dataset\r\n",
    "        targets_cls = batch[:,0]\r\n",
    "        targets = []\r\n",
    "        for target in targets_cls:\r\n",
    "            x = np.zeros(10)\r\n",
    "            x[int(target)] = 1\r\n",
    "            \r\n",
    "            targets.append(x.astype(np.float32))\r\n",
    "        \r\n",
    "        #converting encoded targets to tensor\r\n",
    "        targets = torch.tensor(targets)\r\n",
    "        \r\n",
    "        #putting inputs in model to get preds\r\n",
    "        preds = model(inputs)\r\n",
    "        #putting preds in loss_fn to get loss\r\n",
    "        loss = loss_fn(preds, targets)\r\n",
    "        \r\n",
    "        loss.backward()\r\n",
    "        #reducing the loss \r\n",
    "        opt.step()\r\n",
    "        opt.zero_grad()\r\n",
    "    \r\n",
    "    print(\"Epoch:\",epoch+1,\" Loss:\",round(loss.item(),4))\r\n",
    "    \r\n",
    "    #Accuracy Calcution on the test dataset by unpacking it and aplying trained models\r\n",
    "    correct = 0\r\n",
    "    with torch.no_grad():\r\n",
    "        for batch in test_loader:\r\n",
    "            inputs_test = batch[:,1:785]\r\n",
    "            targets_cls = batch[:,0]\r\n",
    "            \r\n",
    "            pred = model(inputs_test)\r\n",
    "            for i in range(len(batch)):\r\n",
    "                if (pred[i]==pred[i].max()).nonzero().item() == targets_cls[i]:\r\n",
    "                    correct += 1\r\n",
    "        \r\n",
    "        acc = correct/len(test_data)\r\n",
    "        print(\"Accuracy: \",round(acc*100,1),\"%\")\r\n",
    "          "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1  Loss: 0.0882\n",
      "Accuracy:  80.2 %\n",
      "Epoch: 2  Loss: 0.0722\n",
      "Accuracy:  82.8 %\n",
      "Epoch: 3  Loss: 0.089\n",
      "Accuracy:  83.7 %\n",
      "Epoch: 4  Loss: 0.0564\n",
      "Accuracy:  85.0 %\n",
      "Epoch: 5  Loss: 0.0987\n",
      "Accuracy:  84.9 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# The Function to load weights and bias's you like\r\n",
    "\r\n",
    "# torch.save(model.state_dict(),'FashionMNIST-NeuralNetwork.pth')\r\n",
    "\r\n",
    "model_pred = FashionNetwork(784,100,10)\r\n",
    "try:\r\n",
    "    model_pred.load_state_dict(torch.load(\"./saved_models/FashionMNIST-NeuralNetwork.pth\"))\r\n",
    "except:\r\n",
    "    print(\"Error Occured in loading saved weights, using defaults\")\r\n",
    "    model_pred = model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Predict the class using the fully trained model\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "index = 123\r\n",
    "\r\n",
    "test_input = test_data[index,1:785]\r\n",
    "pred = model_pred(test_input)\r\n",
    "\r\n",
    "print(\"Prediction: \",(pred==pred.max()).nonzero().item())\r\n",
    "\r\n",
    "target = test_data[index,0]\r\n",
    "print(\"Target: \", int(target))\r\n",
    "print(\"\"\"\r\n",
    "0 T-shirt/top, \r\n",
    "1 Trouser, \r\n",
    "2 Pullover,\r\n",
    "3 Dress,\r\n",
    "4 Coat,\r\n",
    "5 Sandal,\r\n",
    "6 Shirt,\r\n",
    "7 Sneaker,\r\n",
    "8 Bag,\r\n",
    "9 Ankle boot\r\n",
    "\"\"\")\r\n",
    "\r\n",
    "#converting array to image and seeing it on a graph\r\n",
    "plt.imshow(test_input.reshape(28,28),cmap=\"gray\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:  4\n",
      "Target:  4\n",
      "\n",
      "0 T-shirt/top, \n",
      "1 Trouser, \n",
      "2 Pullover,\n",
      "3 Dress,\n",
      "4 Coat,\n",
      "5 Sandal,\n",
      "6 Shirt,\n",
      "7 Sneaker,\n",
      "8 Bag,\n",
      "9 Ankle boot\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2190076cd30>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "2f97e602bf3efd1202fb41503b5059ddd336053f3958a92ca7b4a7a1e22459bb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}