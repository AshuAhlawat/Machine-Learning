{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dataset = pd.read_csv(\"./fashion-mnist.csv\")\r\n",
    "print(len(dataset))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "targets_cls = dataset.pop(\"label\")\r\n",
    "\r\n",
    "inputs = np.array(dataset)\r\n",
    "targets_cls = np.array(targets_cls)\r\n",
    "\r\n",
    "\r\n",
    "targets = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for _ in targets_cls:\r\n",
    "    x = np.zeros(10)\r\n",
    "    x[_] = 1\r\n",
    "    targets.append(x) \r\n",
    "\r\n",
    "targets = np.array(targets)\r\n",
    "inputs = inputs/255\r\n",
    "\r\n",
    "print(inputs.shape, targets.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs, targets, test_size= 0.2, random_state=123)\r\n",
    "\r\n",
    "print(inputs_train.shape, targets_train.shape, inputs_test.shape, targets_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(48000, 784) (48000, 10) (12000, 784) (12000, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "inputs_train = torch.from_numpy(inputs_train.astype(np.float32)) \r\n",
    "inputs_test = torch.from_numpy(inputs_test.astype(np.float32)) \r\n",
    "targets_train = torch.from_numpy(targets_train.astype(np.float32)) \r\n",
    "targets_test = torch.from_numpy(targets_test.astype(np.float32)) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "input_train_dl = DataLoader(inputs_train,10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class FashionModel(torch.nn.Module):\r\n",
    "    def __init__(self,inputs_dim, layer_dim , output_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.linear1 = torch.nn.Linear(inputs_dim, layer_dim)\r\n",
    "        self.linear2 = torch.nn.Linear(layer_dim, output_dim)\r\n",
    "    \r\n",
    "    def forward(self,inputs):\r\n",
    "        self.output1 = torch.relu(self.linear1(inputs))\r\n",
    "        targets_pred = torch.softmax(self.linear2(self.output1),dim=-1)\r\n",
    "        \r\n",
    "        return(targets_pred)\r\n",
    "\r\n",
    "class FashionModel2(torch.nn.Module):\r\n",
    "    def __init__(self,inputs_dim, output_dim):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = torch.nn.Linear(inputs_dim, output_dim)\r\n",
    "    \r\n",
    "    def forward(self,inputs):\r\n",
    "        targets_pred = torch.softmax(self.linear(inputs),dim=-1)\r\n",
    "        \r\n",
    "        return(targets_pred)\r\n",
    "\r\n",
    "FashionNetwork = FashionModel(784, 64, 10)\r\n",
    "# FashionNetwork = FashionModel2(784,10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "loss_fn = torch.nn.BCELoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "opt = torch.optim.SGD(FashionNetwork.parameters(), lr = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "epochs = 300\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    targets_pred = FashionNetwork(inputs_train)\r\n",
    "    loss = loss_fn(targets_pred, targets_train)\r\n",
    "    \r\n",
    "    loss.backward()\r\n",
    "    \r\n",
    "    opt.step()\r\n",
    "    opt.zero_grad()\r\n",
    "    \r\n",
    "    if (epoch+1)%10 == 0:\r\n",
    "       print(\"Epoch: \", epoch+1, \"  Loss: \", round(loss.item(),4))\r\n",
    "    \r\n",
    "    if (epoch+1)%50 == 0:\r\n",
    "        with torch.no_grad():\r\n",
    "        \r\n",
    "            targets_test_pred = FashionNetwork(inputs_test)\r\n",
    "            targets_test_cls = targets_test_pred.detach().clone()\r\n",
    "\r\n",
    "            for i in range(len(targets_test_cls)):\r\n",
    "                for j in range(len(targets_test_cls[i])):\r\n",
    "                    if targets_test_cls[i][j] == max(targets_test_cls[i]):\r\n",
    "                        targets_test_cls[i][j] = 1\r\n",
    "                    else:\r\n",
    "                        targets_test_cls[i][j] = 0\r\n",
    "            \r\n",
    "            targets_test_temp = targets_test.detach().clone()\r\n",
    "\r\n",
    "            \r\n",
    "            correct = 0\r\n",
    "            \r\n",
    "            for i in range(len(targets_test_temp)):\r\n",
    "                if targets_test_temp[i].eq(targets_test_cls[i]).sum()==10:\r\n",
    "                    correct += 1\r\n",
    "            \r\n",
    "            acc = correct/len(targets_test)\r\n",
    "            accuracy = round(acc*100)\r\n",
    "            \r\n",
    "            print(\"Accuracy : \",accuracy,\"%\")\r\n",
    "            \r\n",
    "            if accuracy>90:\r\n",
    "                break "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  1   Loss:  0.3276\n",
      "Epoch:  2   Loss:  0.3218\n",
      "Epoch:  3   Loss:  0.3178\n",
      "Epoch:  4   Loss:  0.3137\n",
      "Epoch:  5   Loss:  0.3096\n",
      "Epoch:  6   Loss:  0.3055\n",
      "Epoch:  7   Loss:  0.301\n",
      "Epoch:  8   Loss:  0.2962\n",
      "Epoch:  9   Loss:  0.2912\n",
      "Epoch:  10   Loss:  0.2858\n",
      "Epoch:  11   Loss:  0.28\n",
      "Epoch:  12   Loss:  0.274\n",
      "Epoch:  13   Loss:  0.2677\n",
      "Epoch:  14   Loss:  0.2613\n",
      "Epoch:  15   Loss:  0.2546\n",
      "Epoch:  16   Loss:  0.248\n",
      "Epoch:  17   Loss:  0.2414\n",
      "Epoch:  18   Loss:  0.2349\n",
      "Epoch:  19   Loss:  0.2286\n",
      "Epoch:  20   Loss:  0.2226\n",
      "Epoch:  21   Loss:  0.2168\n",
      "Epoch:  22   Loss:  0.2114\n",
      "Epoch:  23   Loss:  0.2062\n",
      "Epoch:  24   Loss:  0.2013\n",
      "Epoch:  25   Loss:  0.1968\n",
      "Epoch:  26   Loss:  0.1925\n",
      "Epoch:  27   Loss:  0.1885\n",
      "Epoch:  28   Loss:  0.1847\n",
      "Epoch:  29   Loss:  0.1812\n",
      "Epoch:  30   Loss:  0.178\n",
      "Epoch:  31   Loss:  0.1749\n",
      "Epoch:  32   Loss:  0.172\n",
      "Epoch:  33   Loss:  0.1694\n",
      "Epoch:  34   Loss:  0.1669\n",
      "Epoch:  35   Loss:  0.1645\n",
      "Epoch:  36   Loss:  0.1623\n",
      "Epoch:  37   Loss:  0.1603\n",
      "Epoch:  38   Loss:  0.1584\n",
      "Epoch:  39   Loss:  0.1565\n",
      "Epoch:  40   Loss:  0.1548\n",
      "Epoch:  41   Loss:  0.1532\n",
      "Epoch:  42   Loss:  0.1517\n",
      "Epoch:  43   Loss:  0.1503\n",
      "Epoch:  44   Loss:  0.1489\n",
      "Epoch:  45   Loss:  0.1476\n",
      "Epoch:  46   Loss:  0.1464\n",
      "Epoch:  47   Loss:  0.1452\n",
      "Epoch:  48   Loss:  0.1441\n",
      "Epoch:  49   Loss:  0.143\n",
      "Epoch:  50   Loss:  0.142\n",
      "Accuracy :  69 %\n",
      "Epoch:  51   Loss:  0.141\n",
      "Epoch:  52   Loss:  0.1401\n",
      "Epoch:  53   Loss:  0.1393\n",
      "Epoch:  54   Loss:  0.1392\n",
      "Epoch:  55   Loss:  0.1415\n",
      "Epoch:  56   Loss:  0.1491\n",
      "Epoch:  57   Loss:  0.1548\n",
      "Epoch:  58   Loss:  0.1452\n",
      "Epoch:  59   Loss:  0.1491\n",
      "Epoch:  60   Loss:  0.1549\n",
      "Epoch:  61   Loss:  0.1481\n",
      "Epoch:  62   Loss:  0.1412\n",
      "Epoch:  63   Loss:  0.136\n",
      "Epoch:  64   Loss:  0.1376\n",
      "Epoch:  65   Loss:  0.1414\n",
      "Epoch:  66   Loss:  0.1433\n",
      "Epoch:  67   Loss:  0.1371\n",
      "Epoch:  68   Loss:  0.1386\n",
      "Epoch:  69   Loss:  0.1409\n",
      "Epoch:  70   Loss:  0.1371\n",
      "Epoch:  71   Loss:  0.1359\n",
      "Epoch:  72   Loss:  0.1381\n",
      "Epoch:  73   Loss:  0.1359\n",
      "Epoch:  74   Loss:  0.1335\n",
      "Epoch:  75   Loss:  0.1354\n",
      "Epoch:  76   Loss:  0.1344\n",
      "Epoch:  77   Loss:  0.1327\n",
      "Epoch:  78   Loss:  0.1329\n",
      "Epoch:  79   Loss:  0.1327\n",
      "Epoch:  80   Loss:  0.1314\n",
      "Epoch:  81   Loss:  0.1313\n",
      "Epoch:  82   Loss:  0.1309\n",
      "Epoch:  83   Loss:  0.1303\n",
      "Epoch:  84   Loss:  0.1297\n",
      "Epoch:  85   Loss:  0.1295\n",
      "Epoch:  86   Loss:  0.1288\n",
      "Epoch:  87   Loss:  0.1285\n",
      "Epoch:  88   Loss:  0.128\n",
      "Epoch:  89   Loss:  0.1277\n",
      "Epoch:  90   Loss:  0.1271\n",
      "Epoch:  91   Loss:  0.1268\n",
      "Epoch:  92   Loss:  0.1263\n",
      "Epoch:  93   Loss:  0.126\n",
      "Epoch:  94   Loss:  0.1254\n",
      "Epoch:  95   Loss:  0.1252\n",
      "Epoch:  96   Loss:  0.1246\n",
      "Epoch:  97   Loss:  0.1244\n",
      "Epoch:  98   Loss:  0.1239\n",
      "Epoch:  99   Loss:  0.1237\n",
      "Epoch:  100   Loss:  0.1231\n",
      "Accuracy :  73 %\n",
      "Epoch:  101   Loss:  0.1229\n",
      "Epoch:  102   Loss:  0.1224\n",
      "Epoch:  103   Loss:  0.1222\n",
      "Epoch:  104   Loss:  0.1217\n",
      "Epoch:  105   Loss:  0.1215\n",
      "Epoch:  106   Loss:  0.121\n",
      "Epoch:  107   Loss:  0.1208\n",
      "Epoch:  108   Loss:  0.1203\n",
      "Epoch:  109   Loss:  0.1202\n",
      "Epoch:  110   Loss:  0.1196\n",
      "Epoch:  111   Loss:  0.1195\n",
      "Epoch:  112   Loss:  0.1189\n",
      "Epoch:  113   Loss:  0.1189\n",
      "Epoch:  114   Loss:  0.1183\n",
      "Epoch:  115   Loss:  0.1183\n",
      "Epoch:  116   Loss:  0.1177\n",
      "Epoch:  117   Loss:  0.1177\n",
      "Epoch:  118   Loss:  0.1171\n",
      "Epoch:  119   Loss:  0.1171\n",
      "Epoch:  120   Loss:  0.1165\n",
      "Epoch:  121   Loss:  0.1165\n",
      "Epoch:  122   Loss:  0.1159\n",
      "Epoch:  123   Loss:  0.1159\n",
      "Epoch:  124   Loss:  0.1153\n",
      "Epoch:  125   Loss:  0.1154\n",
      "Epoch:  126   Loss:  0.1147\n",
      "Epoch:  127   Loss:  0.1148\n",
      "Epoch:  128   Loss:  0.1142\n",
      "Epoch:  129   Loss:  0.1143\n",
      "Epoch:  130   Loss:  0.1136\n",
      "Epoch:  131   Loss:  0.1138\n",
      "Epoch:  132   Loss:  0.1131\n",
      "Epoch:  133   Loss:  0.1133\n",
      "Epoch:  134   Loss:  0.1125\n",
      "Epoch:  135   Loss:  0.1129\n",
      "Epoch:  136   Loss:  0.112\n",
      "Epoch:  137   Loss:  0.1125\n",
      "Epoch:  138   Loss:  0.1114\n",
      "Epoch:  139   Loss:  0.1122\n",
      "Epoch:  140   Loss:  0.1108\n",
      "Epoch:  141   Loss:  0.1119\n",
      "Epoch:  142   Loss:  0.1102\n",
      "Epoch:  143   Loss:  0.1116\n",
      "Epoch:  144   Loss:  0.1097\n",
      "Epoch:  145   Loss:  0.1114\n",
      "Epoch:  146   Loss:  0.1091\n",
      "Epoch:  147   Loss:  0.111\n",
      "Epoch:  148   Loss:  0.1086\n",
      "Epoch:  149   Loss:  0.1104\n",
      "Epoch:  150   Loss:  0.1081\n",
      "Accuracy :  77 %\n",
      "Epoch:  151   Loss:  0.1097\n",
      "Epoch:  152   Loss:  0.1077\n",
      "Epoch:  153   Loss:  0.1089\n",
      "Epoch:  154   Loss:  0.1073\n",
      "Epoch:  155   Loss:  0.1081\n",
      "Epoch:  156   Loss:  0.107\n",
      "Epoch:  157   Loss:  0.1072\n",
      "Epoch:  158   Loss:  0.1067\n",
      "Epoch:  159   Loss:  0.1063\n",
      "Epoch:  160   Loss:  0.1065\n",
      "Epoch:  161   Loss:  0.1054\n",
      "Epoch:  162   Loss:  0.1064\n",
      "Epoch:  163   Loss:  0.1044\n",
      "Epoch:  164   Loss:  0.1065\n",
      "Epoch:  165   Loss:  0.1035\n",
      "Epoch:  166   Loss:  0.107\n",
      "Epoch:  167   Loss:  0.1033\n",
      "Epoch:  168   Loss:  0.1083\n",
      "Epoch:  169   Loss:  0.1062\n",
      "Epoch:  170   Loss:  0.1089\n",
      "Epoch:  171   Loss:  0.1124\n",
      "Epoch:  172   Loss:  0.1056\n",
      "Epoch:  173   Loss:  0.1095\n",
      "Epoch:  174   Loss:  0.1069\n",
      "Epoch:  175   Loss:  0.1089\n",
      "Epoch:  176   Loss:  0.1053\n",
      "Epoch:  177   Loss:  0.1084\n",
      "Epoch:  178   Loss:  0.1052\n",
      "Epoch:  179   Loss:  0.1072\n",
      "Epoch:  180   Loss:  0.1046\n",
      "Epoch:  181   Loss:  0.1067\n",
      "Epoch:  182   Loss:  0.1041\n",
      "Epoch:  183   Loss:  0.106\n",
      "Epoch:  184   Loss:  0.1037\n",
      "Epoch:  185   Loss:  0.1054\n",
      "Epoch:  186   Loss:  0.1033\n",
      "Epoch:  187   Loss:  0.1049\n",
      "Epoch:  188   Loss:  0.1029\n",
      "Epoch:  189   Loss:  0.1044\n",
      "Epoch:  190   Loss:  0.1025\n",
      "Epoch:  191   Loss:  0.1039\n",
      "Epoch:  192   Loss:  0.1021\n",
      "Epoch:  193   Loss:  0.1034\n",
      "Epoch:  194   Loss:  0.1018\n",
      "Epoch:  195   Loss:  0.103\n",
      "Epoch:  196   Loss:  0.1014\n",
      "Epoch:  197   Loss:  0.1026\n",
      "Epoch:  198   Loss:  0.1011\n",
      "Epoch:  199   Loss:  0.1022\n",
      "Epoch:  200   Loss:  0.1008\n",
      "Accuracy :  78 %\n",
      "Epoch:  201   Loss:  0.1018\n",
      "Epoch:  202   Loss:  0.1004\n",
      "Epoch:  203   Loss:  0.1015\n",
      "Epoch:  204   Loss:  0.1001\n",
      "Epoch:  205   Loss:  0.1011\n",
      "Epoch:  206   Loss:  0.0998\n",
      "Epoch:  207   Loss:  0.1008\n",
      "Epoch:  208   Loss:  0.0996\n",
      "Epoch:  209   Loss:  0.1004\n",
      "Epoch:  210   Loss:  0.0993\n",
      "Epoch:  211   Loss:  0.1001\n",
      "Epoch:  212   Loss:  0.099\n",
      "Epoch:  213   Loss:  0.0997\n",
      "Epoch:  214   Loss:  0.0987\n",
      "Epoch:  215   Loss:  0.0994\n",
      "Epoch:  216   Loss:  0.0985\n",
      "Epoch:  217   Loss:  0.0991\n",
      "Epoch:  218   Loss:  0.0982\n",
      "Epoch:  219   Loss:  0.0988\n",
      "Epoch:  220   Loss:  0.098\n",
      "Epoch:  221   Loss:  0.0985\n",
      "Epoch:  222   Loss:  0.0978\n",
      "Epoch:  223   Loss:  0.0981\n",
      "Epoch:  224   Loss:  0.0976\n",
      "Epoch:  225   Loss:  0.0977\n",
      "Epoch:  226   Loss:  0.0974\n",
      "Epoch:  227   Loss:  0.0973\n",
      "Epoch:  228   Loss:  0.0973\n",
      "Epoch:  229   Loss:  0.0969\n",
      "Epoch:  230   Loss:  0.0973\n",
      "Epoch:  231   Loss:  0.0963\n",
      "Epoch:  232   Loss:  0.0973\n",
      "Epoch:  233   Loss:  0.0956\n",
      "Epoch:  234   Loss:  0.0975\n",
      "Epoch:  235   Loss:  0.0948\n",
      "Epoch:  236   Loss:  0.0981\n",
      "Epoch:  237   Loss:  0.0947\n",
      "Epoch:  238   Loss:  0.0998\n",
      "Epoch:  239   Loss:  0.0991\n",
      "Epoch:  240   Loss:  0.1031\n",
      "Epoch:  241   Loss:  0.1117\n",
      "Epoch:  242   Loss:  0.1027\n",
      "Epoch:  243   Loss:  0.1041\n",
      "Epoch:  244   Loss:  0.1031\n",
      "Epoch:  245   Loss:  0.1064\n",
      "Epoch:  246   Loss:  0.0989\n",
      "Epoch:  247   Loss:  0.0992\n",
      "Epoch:  248   Loss:  0.0975\n",
      "Epoch:  249   Loss:  0.0997\n",
      "Epoch:  250   Loss:  0.097\n",
      "Accuracy :  80 %\n",
      "Epoch:  251   Loss:  0.0983\n",
      "Epoch:  252   Loss:  0.0962\n",
      "Epoch:  253   Loss:  0.0976\n",
      "Epoch:  254   Loss:  0.0958\n",
      "Epoch:  255   Loss:  0.0971\n",
      "Epoch:  256   Loss:  0.0955\n",
      "Epoch:  257   Loss:  0.0966\n",
      "Epoch:  258   Loss:  0.0952\n",
      "Epoch:  259   Loss:  0.0963\n",
      "Epoch:  260   Loss:  0.0949\n",
      "Epoch:  261   Loss:  0.096\n",
      "Epoch:  262   Loss:  0.0947\n",
      "Epoch:  263   Loss:  0.0957\n",
      "Epoch:  264   Loss:  0.0944\n",
      "Epoch:  265   Loss:  0.0954\n",
      "Epoch:  266   Loss:  0.0942\n",
      "Epoch:  267   Loss:  0.0951\n",
      "Epoch:  268   Loss:  0.094\n",
      "Epoch:  269   Loss:  0.0949\n",
      "Epoch:  270   Loss:  0.0938\n",
      "Epoch:  271   Loss:  0.0946\n",
      "Epoch:  272   Loss:  0.0935\n",
      "Epoch:  273   Loss:  0.0943\n",
      "Epoch:  274   Loss:  0.0933\n",
      "Epoch:  275   Loss:  0.0941\n",
      "Epoch:  276   Loss:  0.0931\n",
      "Epoch:  277   Loss:  0.0938\n",
      "Epoch:  278   Loss:  0.0929\n",
      "Epoch:  279   Loss:  0.0936\n",
      "Epoch:  280   Loss:  0.0927\n",
      "Epoch:  281   Loss:  0.0933\n",
      "Epoch:  282   Loss:  0.0925\n",
      "Epoch:  283   Loss:  0.0931\n",
      "Epoch:  284   Loss:  0.0923\n",
      "Epoch:  285   Loss:  0.0929\n",
      "Epoch:  286   Loss:  0.0922\n",
      "Epoch:  287   Loss:  0.0926\n",
      "Epoch:  288   Loss:  0.092\n",
      "Epoch:  289   Loss:  0.0924\n",
      "Epoch:  290   Loss:  0.0918\n",
      "Epoch:  291   Loss:  0.0922\n",
      "Epoch:  292   Loss:  0.0917\n",
      "Epoch:  293   Loss:  0.092\n",
      "Epoch:  294   Loss:  0.0915\n",
      "Epoch:  295   Loss:  0.0917\n",
      "Epoch:  296   Loss:  0.0913\n",
      "Epoch:  297   Loss:  0.0915\n",
      "Epoch:  298   Loss:  0.0912\n",
      "Epoch:  299   Loss:  0.0913\n",
      "Epoch:  300   Loss:  0.091\n",
      "Accuracy :  81 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "index = 1111\r\n",
    "\r\n",
    "test = inputs[index]\r\n",
    "\r\n",
    "img_arr = test.reshape(28,28)\r\n",
    "plt.imshow(img_arr, cmap=\"gray\")\r\n",
    "\r\n",
    "test = torch.from_numpy(test.reshape(1,-1).astype(np.float32))\r\n",
    "\r\n",
    "pred = FashionNetwork(test).detach().numpy()\r\n",
    "pred_cls = np.where(pred==pred.max())\r\n",
    "print(\"Prediction: \",pred_cls[1][0])\r\n",
    "print(\"Target: \",targets_cls[index])\r\n",
    "\r\n",
    "print(\"\"\"\r\n",
    "0 T-shirt/top, 1 Trouser, \r\n",
    "2 Pullover,\r\n",
    "3 Dress,\r\n",
    "4 Coat,\r\n",
    "5 Sandal,\r\n",
    "6 Shirt,\r\n",
    "7 Sneaker,\r\n",
    "8 Bag,\r\n",
    "9 Ankle boot\r\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:  9\n",
      "Target:  9\n",
      "\n",
      "0 T-shirt/top, 1 Trouser, \n",
      "2 Pullover,\n",
      "3 Dress,\n",
      "4 Coat,\n",
      "5 Sandal,\n",
      "6 Shirt,\n",
      "7 Sneaker,\n",
      "8 Bag,\n",
      "9 Ankle boot\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "2f97e602bf3efd1202fb41503b5059ddd336053f3958a92ca7b4a7a1e22459bb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}