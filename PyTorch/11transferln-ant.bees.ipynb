{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import torchvision.transforms as tfms\n",
    "\n",
    "mean = (0.5225, 0.483, 0.3616)\n",
    "std = (0.2775, 0.2593, 0.2915)\n",
    "\n",
    "train_tfms = tfms.Compose([\n",
    "    #already resized to 300x300\n",
    "    tfms.RandomResizedCrop(256),\n",
    "    tfms.RandomHorizontalFlip(),\n",
    "    tfms.ToTensor(),\n",
    "    tfms.Normalize(mean,std)\n",
    "])\n",
    "\n",
    "val_tfms = tfms.Compose([\n",
    "    tfms.Resize(300),\n",
    "    tfms.CenterCrop(256),\n",
    "    tfms.ToTensor(),\n",
    "    tfms.Normalize(mean,std)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "root = \"./data/hymenoptera/\"\n",
    "\n",
    "train_ds = ImageFolder(root+\"train\", train_tfms)\n",
    "val_ds = ImageFolder(root+\"val\", val_tfms)\n",
    "\n",
    "img_cls = train_ds.classes\n",
    "print(img_cls)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ants', 'bees']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_ds, 50,shuffle=True,pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, 20,shuffle=False,pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "# grid = ImageGrid(fig,111,(5,5))\n",
    "\n",
    "\n",
    "# for images,labels in val_dl:\n",
    "#     image_arr = images.permute(0,2,3,1)\n",
    "\n",
    "#     for axis,image in zip(grid,image_arr):\n",
    "#         axis.imshow(image)\n",
    "#     plt.show()\n",
    "#     break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.norm15 = nn.BatchNorm2d(15)\n",
    "#         self.norm60 = nn.BatchNorm2d(60)\n",
    "#         self.norm120 = nn.BatchNorm2d(120)\n",
    "#         self.norm240 = nn.BatchNorm2d(240)\n",
    "        \n",
    "        \n",
    "#         self.conv15 = nn.Conv2d(3,15,3,1,1)\n",
    "        \n",
    "#         self.conv60a = nn.Conv2d(15,60,3,1,1)\n",
    "#         self.conv60b = nn.Conv2d(60,60,3,1,1)\n",
    "        \n",
    "#         self.conv120a = nn.Conv2d(60,120,3,1,1)\n",
    "#         self.conv120b = nn.Conv2d(120,120,3,1,1)\n",
    "#         self.conv120c = nn.Conv2d(120,120,3,1,1)\n",
    "        \n",
    "#         self.conv240a = nn.Conv2d(120,240,3,1,1)\n",
    "#         self.conv240b = nn.Conv2d(240,240,3,1,1)\n",
    "#         self.conv240c = nn.Conv2d(240,240,3,1,1)\n",
    "        \n",
    "        \n",
    "#         self.pool = nn.MaxPool2d(2,2)\n",
    "#         self.aapool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "#         self.flat = nn.Flatten()\n",
    "        \n",
    "#         self.linear = nn.Linear(240,2)\n",
    "        \n",
    "    \n",
    "#     def forward(self,data):\n",
    "#         out = torch.relu(self.norm15(self.conv15(data)))# 15 256 256\n",
    "#         out = self.pool(out)#15 128 128\n",
    "        \n",
    "#         out = torch.relu(self.norm60(self.conv60a(out)))# 60 128 128\n",
    "#         x = out\n",
    "#         out = torch.relu(self.conv60b(out)+x)\n",
    "        \n",
    "#         out = self.pool(out)#60 64 64\n",
    "        \n",
    "#         x = self.norm120(self.conv240a(out))#120 64 64\n",
    "#         out = torch.relu(x)\n",
    "#         out = torch.relu(self.conv120b(out)+x)\n",
    "#         out = torch.relu(self.conv120c(out))\n",
    "        \n",
    "#         out = self.pool(out)#120 32 32\n",
    "        \n",
    "#         x = self.norm240(self.conv240a(out))# 240 32 32\n",
    "#         out = torch.relu(x)\n",
    "#         out = torch.relu(self.conv240b(out)+x)\n",
    "#         out = torch.relu(self.conv240c(out))\n",
    "        \n",
    "#         out = self.aapool(out)#240 1 1 \n",
    "        \n",
    "#         out = self.flat(out)#240\n",
    "#         out = self.linear(out)#2\n",
    "        \n",
    "#         out = torch.softmax(out, dim=-1)\n",
    "            \n",
    "#         return out\n",
    "        \n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=1)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs,2)\n",
    "\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "print(len(val_dl))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: \",epoch+1)\n",
    "\n",
    "        for images,labels in train_dl:\n",
    "            preds = model(images.to(device))\n",
    "                \n",
    "            loss = loss_fn(preds.to(device), labels.to(device))\n",
    "            loss.backward()\n",
    "                \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            break\n",
    "\n",
    "        print(\"Loss: \",round(loss.item(),6))\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            for images,labels in val_dl:\n",
    "                preds = model(images.to(device))\n",
    "                \n",
    "                for i in range(len(preds)):\n",
    "                    if (preds[i].max()==preds[i][labels[i].item()]):\n",
    "                        correct += 1\n",
    "                \n",
    "            acc = correct/len(val_ds)\n",
    "            print(\"Accuracy: \",round(acc*100,2))\n",
    "            print(\"\")\n",
    "        if acc>0.80:\n",
    "            break\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "fit(20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  1\n",
      "Loss:  0.244928\n",
      "Accuracy:  0.5875\n",
      "\n",
      "Epoch:  2\n",
      "Loss:  0.170394\n",
      "Accuracy:  0.6\n",
      "\n",
      "Epoch:  3\n",
      "Loss:  0.416226\n",
      "Accuracy:  0.575\n",
      "\n",
      "Epoch:  4\n",
      "Loss:  0.15215\n",
      "Accuracy:  0.5625\n",
      "\n",
      "Epoch:  5\n",
      "Loss:  0.214524\n",
      "Accuracy:  0.55\n",
      "\n",
      "Epoch:  6\n",
      "Loss:  0.214051\n",
      "Accuracy:  0.525\n",
      "\n",
      "Epoch:  7\n",
      "Loss:  0.137485\n",
      "Accuracy:  0.55\n",
      "\n",
      "Epoch:  8\n",
      "Loss:  0.181773\n",
      "Accuracy:  0.5625\n",
      "\n",
      "Epoch:  9\n",
      "Loss:  0.289508\n",
      "Accuracy:  0.5625\n",
      "\n",
      "Epoch:  10\n",
      "Loss:  0.18628\n",
      "Accuracy:  0.575\n",
      "\n",
      "Epoch:  11\n",
      "Loss:  0.136917\n",
      "Accuracy:  0.575\n",
      "\n",
      "Epoch:  12\n",
      "Loss:  0.445\n",
      "Accuracy:  0.5625\n",
      "\n",
      "Epoch:  13\n",
      "Loss:  0.221793\n",
      "Accuracy:  0.55\n",
      "\n",
      "Epoch:  14\n",
      "Loss:  0.148318\n",
      "Accuracy:  0.5125\n",
      "\n",
      "Epoch:  15\n",
      "Loss:  0.269762\n",
      "Accuracy:  0.5\n",
      "\n",
      "Epoch:  16\n",
      "Loss:  0.447253\n",
      "Accuracy:  0.5125\n",
      "\n",
      "Epoch:  17\n",
      "Loss:  0.286475\n",
      "Accuracy:  0.5375\n",
      "\n",
      "Epoch:  18\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}