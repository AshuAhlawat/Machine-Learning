{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torchvision.transforms as tfms\n",
    "\n",
    "mean = (0.5225, 0.483, 0.3616)\n",
    "std = (0.2775, 0.2593, 0.2915)\n",
    "\n",
    "mean = (0.5,0.5,0.5)\n",
    "std = (0.25,0.25,0.25)\n",
    "train_tfms = tfms.Compose([\n",
    "    #already resized to 300x300\n",
    "    tfms.RandomResizedCrop(256),\n",
    "    tfms.RandomHorizontalFlip(),\n",
    "    tfms.ToTensor(),\n",
    "    tfms.Normalize(mean,std)\n",
    "])\n",
    "\n",
    "val_tfms = tfms.Compose([\n",
    "    tfms.Resize(300),\n",
    "    tfms.CenterCrop(256),\n",
    "    tfms.ToTensor(),\n",
    "    tfms.Normalize(mean,std)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "root = \"./data/hymenoptera/\"\n",
    "\n",
    "train_ds = ImageFolder(root+\"train\", train_tfms)\n",
    "val_ds = ImageFolder(root+\"val\", val_tfms)\n",
    "\n",
    "img_cls = train_ds.classes\n",
    "print(img_cls)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ants', 'bees']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_ds, 5,shuffle=True,num_workers=4,pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, 20,shuffle=False,num_workers=4,pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def fit(epochs,model,loss_fn,opt):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: \",epoch+1)\n",
    "\n",
    "        for images,labels in train_dl:\n",
    "            preds = model(images.to(device))\n",
    "        \n",
    "            loss = loss_fn(preds.to(device), labels.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        print(\"Loss: \",round(loss.item(),6))\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            for images,labels in val_dl:\n",
    "                preds = model(images.to(device))\n",
    "                \n",
    "                for i in range(len(preds)):\n",
    "                    if (preds[i].max()==preds[i][labels[i].item()]):\n",
    "                        correct += 1\n",
    "                \n",
    "            acc = correct/len(val_ds)\n",
    "            print(\"Accuracy: \",round(acc*100,2))\n",
    "            print(\"\")\n",
    "        if acc>0.80:\n",
    "            break\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from torchvision import models\n",
    "\n",
    "HymenopteraModel = models.resnet18(pretrained=1)\n",
    "print(HymenopteraModel.fc)\n",
    "\n",
    "for param in HymenopteraModel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = HymenopteraModel.fc.in_features\n",
    "HymenopteraModel.fc = nn.Linear(num_ftrs,2)\n",
    "print(HymenopteraModel.fc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "Linear(in_features=512, out_features=2, bias=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "HymenopteraModel = HymenopteraModel.to(device)\n",
    "CrossEntropy_fn = nn.CrossEntropyLoss()\n",
    "AdamOpt = torch.optim.Adam(HymenopteraModel.parameters(),lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "fit(5,HymenopteraModel,CrossEntropy_fn,AdamOpt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:  1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/capti/.local/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss:  0.368291\n",
      "Accuracy:  53.75\n",
      "\n",
      "Epoch:  2\n",
      "Loss:  0.313351\n",
      "Accuracy:  48.75\n",
      "\n",
      "Epoch:  3\n",
      "Loss:  0.224606\n",
      "Accuracy:  46.25\n",
      "\n",
      "Epoch:  4\n",
      "Loss:  0.486599\n",
      "Accuracy:  46.25\n",
      "\n",
      "Epoch:  5\n",
      "Loss:  0.607188\n",
      "Accuracy:  50.0\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}